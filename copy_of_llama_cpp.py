# -*- coding: utf-8 -*-
"""Copy of llama_cpp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zbs0bDldMOy-cwwSLB5BU7Q_Rr5SZQmt
"""

!git clone https://github.com/ggerganov/llama.cpp.git

# Commented out IPython magic to ensure Python compatibility.
# %cd llama.cpp

!make GGML_OPENBLAS=1

!pip install -r /content/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt

from huggingface_hub import snapshot_download

model_name = "Qwen/Qwen1.5-1.8B"

methods = ['q4_k_m']

base_model = "./original_model/"
quantized_path = "./quantized_model/"

snapshot_download(repo_id=model_name, local_dir=base_model , local_dir_use_symlinks=False)
original_model = quantized_path+'/FP16.gguf'

!mkdir ./quantized_model/

!python /content/llama.cpp/convert_hf_to_gguf.py ./original_model/ --outtype f16 --outfile ./quantized_model/FP16.gguf

import os

for m in methods:
    qtype = f"{quantized_path}/{m.upper()}.gguf"
    os.system("./llama.cpp/quantize "+quantized_path+"/FP16.gguf "+qtype+" "+m)

! /content/llama.cpp/llama-cli -m /content/llama.cpp/quantized_model/FP16.gguf -n 90 --repeat_penalty 1.0 --color -i -r "User:" -f /content/llama.cpp/prompts/chat-with-bob.txt

#Log

from huggingface_hub import notebook_login
notebook_login()

from huggingface_hub import HfApi, HfFolder, create_repo, upload_file

model_path = "/content/llama.cpp/quantized_model/FP16.gguf" # Your model's local path
repo_name = "hawky-ai-qwen1.5-martech-audience-segmentation"  # Desired HF Hub repository name
repo_url = create_repo(repo_name, private=False)

api = HfApi()
api.upload_file(
    path_or_fileobj=model_path,
    path_in_repo="/content/llama.cpp/quantized_model/FP16.gguf",
    repo_id="Sri-Vigneshwar-DJ/hawky-ai-qwen1.5-martech-audience-segmentation",
    repo_type="model",
)

